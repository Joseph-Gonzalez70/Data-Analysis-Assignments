---
title: "HW 4"
author: "Joseph Gonzalez"
date: "5/27/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(dbplyr)
library(class)
library(MASS)
attach(iris)
```

## HW 4

**Problem 1**

For this homework we will consider fisher's iris data set.
```{r}
iris1=iris[which(iris$Species != "setosa"),]
data_versicolor=iris1[which(iris1$Species=="versicolor"),]
data_virginica=iris1[which(iris1$Species=="virginica"),]  
test_data=rbind(data_versicolor[1:10,],data_virginica[1:10,])
train_data=rbind(data_versicolor[11:50,],data_virginica[11:50,])
```

**Problem 2**

We can use the lda() function for classifiication. Sepal length and sepal width are predictors.
```{r}
lda.fit=lda(Species~Sepal.Length+Sepal.Width, data= train_data) #Fit the model
#summary(lda.fit) 
#Classify the test data:
lda_pred<-predict(lda.fit,test_data[,1:2]) 
lda_pred$class #shows binary representation of which class the test data is organized into
lda_pred$posterior #Shows the probability the observation belongs to that class
ifelse(lda_pred$posterior[,2]>0.5,"Virginica","Versicolor") #Species' name classification
```


**Part A**

```{r}
#class specific means of the predictor variables:
#For versicolor:
mean(train_data$Sepal.Length[which(train_data$Species=="versicolor")]) #Mean of sepal length: 5.895 cm
mean(train_data$Sepal.Width[which(train_data$Species=="versicolor")]) #Mean of Sepal Width: 2.745 cm
#For Virginica:
mean(train_data$Sepal.Length[which(train_data$Species=="virginica")]) #Mean of sepal length: 6.5925 cm
mean(train_data$Sepal.Width[which(train_data$Species=="virginica")]) #Mean of Sepal Width   2.9825 cm
```

For versicolor, the mean sepal length is 5.895 cm and the mean sepal width is 2.745 cm. For virginica, the mean sepal length is 6.5925cm and the mean sepal width is 2.9825 cm.

**Part B**

Create a confusion table for the LDA results:

```{r}
confusion=table(Species=test_data$Species,Prediction=lda_pred$class)[2:3,2:3]
confusion
1-sum(diag(confusion))/sum(confusion)
```

The misclassification error rate is 0.4.

**Question 3**

**Part A**

Fit the logistic model to the training data using sepal width and sepal length.

```{r}
#Fit the logistic model using glm
logreg1=glm(Species~Sepal.Length + Sepal.Width, data=train_data, family = binomial)

```

**Part A(i)**

```{r}
#We can show the entire summary of the fit using summary()
(logregsummary=summary(logreg1))
logregsummary$coefficients[,1:2] #This line directly outputs the estimates of the coefficients and standard errors


```
Estimates and standard errors:

$\hat{\beta}_0$= -18.1620126;  $SE[\hat{\beta}_0]=4.3570787$

$\hat{\beta}_1$= 2.5495219;    $SE[\hat{\beta}_0]=0.7097107$

$\hat{\beta}_2$= 0.8173167;   $SE[\hat{\beta}_0]=1.0109006$




**Part A(ii)**

We will form a confusion matrix and the misclassification error rate:

```{r}
prob<-predict(logreg1, test_data, type = "response") #Probability that the test data belongs to a certain group
#We can categorize predictions according to their probability:
predicted<-ifelse(predict(logreg1,test_data, type = "response")<.5,"versicolor","virginica") 
#Generate the confusion table:
confusion=table(predicted,as.character(test_data$Species),dnn = c("Predicted species","True species"))
confusion
#Misclassification error rate:
(accuracy1=sum(diag(confusion))/sum(confusion))
(misclassrate=1-accuracy1)
```
The misclassification error rate is 0.4(40%).

**Part A(iii)**

According to the summary of the logistic regression, both predictors do not seem necessary for classification. Specifically, Sepal.Width is non-significant at a significance levels 0.20,0.10, and 0.05(Its p-value(0.418801)$>$ $\alpha$). Another indicator is that the difference between the null deviance and the residual deviance does not seem to have changed drastically when we have two predictors in the model. We would need to verify this with the residual deviance value of the model with only Sepal.Length as the predictor. lastly, we would also need to see how the confusion matrix and misclassification error rate would change after removing Sepal.Width from the model. If prediction accuracy increases or stays the same, then this would further support that Sepal.Width is not necessary for the model.


**Part B**

In this part, we fit a logistic regression model to the training data using Sepal.Length:

```{r}
logreg2=glm(Species~Sepal.Length, data=train_data, family = binomial)
```

**Part B(i)**
We can use the summary() function to obtain the parameter estimates and standard errors:

```{r}
#Output the summary of the logistic regression with Sepal.Length as the only predictor:
(logregsummary2=summary(logreg2))
#Output the coefficients and standard errors only:
logregsummary2$coefficients[,1:2] 
```

Estimates and standard errors:

$\hat{\beta}_0$= -17.1552  ;  $SE[\hat{\beta}_0] = 4.1251$

$\hat{\beta}_1$= 2.7647;    $SE[\hat{\beta}_0] = 0.6668$



**Part B(ii)**

Confusion Matrix and misclassification error rate:

```{r}
prob2<-predict(logreg2, test_data, type = "response")
predicted2<-ifelse(predict(logreg2,test_data, type = "response")<.5,"versicolor","virginica")
confusion2=table(predicted2,as.character(test_data$Species),dnn = c("Predicted species","True species"))
confusion2
#Misclassification error rate:
(accuracy2=sum(diag(confusion2))/sum(confusion2))
(misclassrate=1-accuracy2)
```

The misclassification error rate is 0.4(40%).

**Part B(iii)**

Compared to 3(a), we first see that the p-value for Sepal.length's estimated coefficient is smaller(more significant) and its standard error is smaller(more accurate). We also see that there a small increase in residual deviance(0.666) and the AIC decreased by 1.334. Since there was only a small change in residual deviance, this means that Sepal.Width had little impact on improving the accuracy of the model(smaller residual deviance more accurate). We also favor models with a smaller AIC because it means less information was lost in the model.

Yes, the result in 3(b)(ii) does support the answer in 3(a)(ii). After removing Sepal.Width from the model, we see that the confusion matrix and the misclassification error rate are the same. This means that Sepal.Width did not influence the classification prediction of the test data. 


**Problem 4**

For this problem, we can use the k-NN method classification method to classify the test data. We will only use sepal length. Also, we will perform the analysis using $k=1$ and $k=5$.

First, we can see when k=1:

```{r}
#First, we have to make sure the input values for the knn() function are classified as a data frame.
train1=as.data.frame(train_data$Sepal.Length) 
test1=as.data.frame(test_data$Sepal.Length)
#Second, we have to ensure that the cl input for the knn() function is classified as a factor
trainclass=as.factor(train_data$Species)
(knnclass1=knn(train1,test1,trainclass,k = 1))
(confusionknn1=table(knnclass1,as.character(test_data$Species))[2:3,])
(accuracy3=sum(diag(confusionknn1))/sum(confusionknn1))
(misclassrate=1-accuracy3)
```

When k=5, the misclassification error rate is 0.35.

Next, we can see when k=5:

```{r}
(knnclass2=knn(train1,test1,trainclass,k = 5))
(confusionknn2=table(knnclass2,as.character(test_data$Species))[2:3,])
(accuracy3=sum(diag(confusionknn2))/sum(confusionknn2))
(misclassrate=1-accuracy3)
```
When k=1, the misclassification error rate is 0.4.

**Problem 5**

For this dataset, each method performed similarly because they produced the same misclassification error rate(0.4). This means that each method had mis-classified the same number of observations. We can see that certain methods would be useful in particular situations(LDA or logistic regression to analyze the predictors). We also saw that, when k=1, knn produced a slightly lower mis-classification error rate than the rest of the methods. While this is compelling, we have to keep in mind that lower k leads to an overly flexible method(high variance) which could be problematic when trying to classify a new set of observations.
