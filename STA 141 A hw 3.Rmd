---
title: "STA 141 HW 3"
author: "Joseph Gonzalez"
date: "5/2/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#Packages
library(ggplot2)
library(MASS)
```

**Problem 1**

Run the code to create the vectors x1, x2, and y.
```{r, include=TRUE}
set.seed(1)
n <- 100
x1 <- runif(n)
x2 <- runif(n,10,20)
y <- 2+2*x1+0.3*x2+rnorm(n)
```

**Part A**

Form of the linear model:

$Y=\beta_0+\beta_1X_1+\beta_2X_2+\mathcal{E}$

$\mathcal{E} \sim N(0,\sigma^2)$

Values of the regression coefficients:

$\beta_0$=2

$\beta_1$=2

$\beta_2$=0.3

Value of $\sigma^2$:

```{r, include=TRUE}
set.seed(1)
(sigma=sum(rnorm(n)^2)/(n-3))
```


$\sigma^2= \frac{||e||^2}{n-p-1} =0.8356195$

**Part B**

For the correlation coefficient, we can use the cor() function. We can also use the ggplot function to visually represent their relationship.
```{r, include=TRUE}
cor(x1,x2) #Correlation between x1 and x2
data1=data.frame(cbind(x1,x2,y)) #Data frame for ggplot
ggplot(data1[,1:2], mapping=aes(x1,x2))+geom_point()+labs(title = "X2 vs. X1",x = "X1",y = "X2")
```

The correlation coefficient $r=0.01703215$


**Part C**

```{r, include=TRUE}
fit1=lm(y~x1+x2, data=data1)
summary(fit1)
```

Values of the coefficients:

$\hat\beta_0$=  1.97628  

$\hat\beta_1$=  1.93074  

$\hat\beta_2$= 0.30144 

The explanatory variables seem to have a positive relationship with the response variable. We can make this assumption since the coefficients are positive. The estimate $\hat\beta_0$ underestimates $\beta_0$ by 0.02372, $\hat\beta_1$ underestimates $\beta_1$ by 0.06926, $\hat\beta_2$ overestimates $\beta_2$ by .00144. Overall, these estimates are very close to the true population parameters. The value of s=0.9675 and $s^2$=0.9361. The estimate $s^2$ over estimates $\sigma^2$. According to the t-value(5.312) and p-value(6.89e-07), we can reject the null hypothesis $H_0:\beta_1=0$ at a $\alpha$=0.001 significance level. According to the t-vaue(8.425) and p-value(3.33e-13), we can also reject the null hypothesis $H_0:\beta_2=0$ at a $\alpha$=0.001 significance level. 

**Part D**

```{r, include=TRUE}
fit2=lm(y~x1, data=data1)
summary(fit2)
```

The estimates of coefficients:

$\hat\beta_0$=  6.5235 

$\hat\beta_1$=  1.9829

The explanatory variable(x1) seems to have a positive relationship with the response variable. We can make this assumption since the coefficient is positive. The estimate $\hat\beta_0$ over-estimates $\beta_0$ by 4.5235. The estimate $\hat\beta_1$ under-estimates $\beta_1$ by 0.0171. The estimate s=1.267 and $s^2$=1.605. $s^2$ over-estimates $\sigma^2$. According to the t-value(4.168) and p-value(6.64e-05), we can reject the null hypothesis $H_0:\beta_1=0$ at a $\alpha$=0.001 significance level. 

**Part E**
```{r, include=TRUE}
fit3=lm(y~x2, data=data1)
summary(fit3)
```
The estimates of coefficients:

$\hat\beta_0$=  2.92698  

$\hat\beta_2$=  0.30467   

The explanatory variable(x2) seems to have a positive relationship with the response variable. We can make this assumption since the coefficient is positive. The estimate $\hat\beta_0$ over-estimates $\beta_0$ by 0.92698. The estimate $\hat\beta_2$ over-estimates $\beta_2$ by 0.00467. The value of $s$ is 1.094 and $s^2$ is 1.196836. Both values over-estimate $\sigma$ and $\sigma^2$. According to the t-value(7.536) and p-value(2.46e-11), we can reject the null hypothesis $H_0:\beta_2=0$ at a $\alpha$=0.001 significance level. 

---------------------------------------------------------------------------------------------------
**Problem 2**

Create the vectors x1, x2, and y:
```{r, include=TRUE}
set.seed(1)
n <- 100
x1 <- runif(n)
x2 <- 0.5*x1+rnorm(n,0,.01)
y <- 2+2*x1+0.3*x2+rnorm(n)
```

**Part a**

Repeat parts b, c, d, abd e of exercise 1:

**Part b**
```{r}
cor(x1,x2)
data2=data.frame(cbind(x1,x2,y))
ggplot(data1[,1:2], mapping=aes(x1,x2))+geom_point()+labs(title = "X2 vs. X1",x = "X1",y = "X2")
```

Compared to problem one, x1 and x2 are more correlated with each other(r=0.998 compared to 0.01703). This occurs since x2 is a function of x1.

**Part C**
```{r, include=TRUE}
fit1=lm(y~x1+x2, data=data2)
summary(fit1)
```

Values of the coefficients:

$\hat\beta_0$= 2.1305 

$\hat\beta_1$= -1.7540   

$\hat\beta_2$= 7.3967 

In this model, the variables(Individually) do not seem to have a relationship with the response variable. The estimate $\hat\beta_0$ over-estimates $\beta_0$ by 0.1305, $\hat\beta_1$ underestimates $\beta_1$ by 3.754, $\hat\beta_2$ overestimates $\beta_2$ by 7.0967. In contrast to problem 1, these estimates are very far away from the true population parameters. The value of $s$ is 1.056 and $s^2$ is 1.115. The estimate $s^2$ over estimates $\sigma^2$. According to the t-value(-0.307) and p-value(0.760), we can not reject the null hypothesis $H_0:\beta_1=0$ at a $\alpha$=0.1 significance level. According to the t-vaue(0.652) and p-value(0.516), we can not reject the null hypothesis $H_0:\beta_2=0$ at a $\alpha$=0.1 significance level. The estimates of $\beta_1$ and $\beta_2$ are much different than the estimates from 1c. Compared to 1c, $\hat\beta_1$ is negative and $\hat\beta_2$ is much larger. 


**Part D**
```{r, include=TRUE}
fit2=lm(y~x1, data=data2)
summary(fit2)
```

The estimates of coefficients:

$\hat\beta_0$= 2.1172 

$\hat\beta_1$= 1.9675 

In this model, the variable(x1) does seem to have a positive relationship with the response variable. The estimate $\hat\beta_0$ over-estimates $\beta_0$ by 0.1172. The estimate $\hat\beta_1$ under-estimates $\beta_1$ by 0.0325. The estimate s=1.053 and $s^2$=1.109. $s^2$ over-estimates $\sigma^2$. According to the t-value(4.974) and p-value(2.79e-06), we can reject the null hypothesis $H_0:\beta_1=0$ at a $\alpha$=0.001 significance level. Compared to 1d, the estimate for $\beta_1$ is about the same.

**Part E**
```{r, include=TRUE}
fit3=lm(y~x2, data=data2)
summary(fit3)
```

The estimates of coefficients:

$\hat\beta_0$=2.1199  

$\hat\beta_2$=3.9273  

In this model, the variable(x2) seems to have a positive relationship with the response variable. The estimate $\hat\beta_0$ over-estimates $\beta_0$ by 0.1199. The estimate $\hat\beta_2$ over-estimates $\beta_2$ by 3.6273. The value of $s$ is 1.051 and $s^2$ is 1.105. Both values over-estimate $\sigma$ and $\sigma^2$. According to the t-value(5.016) and p-value(2.35e-06), we can reject the null hypothesis $H_0:\beta_2=0$ at a $\alpha$=0.001 significance level. Compared to 1e, the estimate for $\beta_2$ is larger in this problem.

**Why do these differences occur?**

These differences occur because the variables, x1 and x2, are highly correlated in problem 2 and were lowly correlated in problem 1. When explanatory variables are highly correlated and within the same model, multicollinearity occurs. This means that there are many subsets of the coefficients that satisfy the least square criterion. The effect of one explanatory variable on the response variable highly depends on the other explanatory variable. Together the coefficients are not meaningful and, separately, we see they have significance. We can also see that the F-test is significant and the t-tests are not in the full model. This is saying, in the full model, that the marginal effect of each variable is not significant but there is signicance between the response variable and the group(x1,x2) of explanatory variables.

**Problem 3**
Add a new observation:

```{r, include=TRUE}
x1=c(x1,0.1)
x2=c(x2,0.8)
y=c(y,6)
data3=data.frame(cbind(x1,x2,y))

```

Re-fit models from 1c, 1d, and 1e using new data. We want to evaluate the effect of the new observation on the models.

**Part C**
```{r, include=TRUE}
fit1=lm(y~x1+x2, data=data3)
summary(fit1)
plot(fit1)
```

Effect of the new observation: Compared to problem 2, we see now that the estimate for $\beta_2$ is now significant(Was insignificant in the full model for problem 2).

We want to know if the observation is an outlier and/or a high leverage point.

For high-leverage points, we need to find $\frac{2p}{n}$ and the leverage values to use the high-leverage value criterion: $h_{ii}>\frac{2p}{n}$:

```{r, include=TRUE}
lbound=(2*length(fit1$coefficients))/101
leverage=hatvalues(fit1)
which(leverage>lbound)
#Influential point
cooksd=cooks.distance(fit1)
influen=4/(101-length(fit1$coefficients))
which(cooksd>influen)
plot(cooksd, main ="Cook's distance", ylab="Cook's distance")
abline(h = 4/(101-length(fit1$coefficients)), col="red")
#Outlying in Y using studentized deleted Residuals
stu.res.del=studres(fit1)
head(sort(abs(stu.res.del),decreasing=TRUE))
#Compare Values to the Bonferroni Threshold
qt(1-.1/(2*101), 101-3-1)
```

According to the criterion and the "Residuals vs. Leverage plot" for model 1, the new observation(101) is a high leverage point because its leverage value does satisfy $h_{ii}>\frac{2P}{n}$. It is not an outlier in Y because it lies within 3 standard deviations of zero and its studentized deleted residual does not exceed the Bonferroni threshold. The new observation is also an influential point because, according to cooks distance, its influence is more than $\frac{4}{n-p}$

**Part D**
```{r, include=TRUE}
fit2=lm(y~x1, data=data3)
summary(fit2)
plot(fit2)
```

Effect of the new observation: Compared to problem 2, the R-squared values are smaller and the coefficient for x1 is a bit smaller.

For high-leverage points, we need to find $\frac{2p}{n}$ and the leverage values to use the high-leverage value criterion: $h_{ii}>\frac{2p}{n}$:

```{r, include=TRUE}
lbound=(2*length(fit2$coefficients))/101
leverage=hatvalues(fit2)
which(leverage>lbound)
#Influential point
cooksd=cooks.distance(fit2)
influen=4/(101-length(fit2$coefficients))
which(cooksd>influen)
plot(cooksd, main ="Cook's distance", ylab="Cook's distance")
abline(h = 4/(101-length(fit2$coefficients)), col="red")
#Outlying in Y using studentized deleted Residuals
stu.res.del=studres(fit2)
head(sort(abs(stu.res.del),decreasing=TRUE))
#Compare Values to the Bonferroni Threshold
qt(1-.1/(2*101), 101-2-1)
```


According to the criterion and the "Residuals vs. Leverage plot" for model 2, the new observation(101) is not a leverage point because its leverage value does not satisfy $h_{ii}>\frac{2p}{n}$. It is an outlier in Y because it lies outside 3 standard deviations of zero and its studentized deleted residual is larger than the Bonferroni threshold. The new observation is also an influential point according to cooks distance(its influence is more than $\frac{4}{n-p}$).


**Part E**
```{r, include=TRUE}
fit3=lm(y~x2, data=data3)
summary(fit3)
plot(fit3)
```

Effect of the new observation: Compared to problem 2, the $R^2$ values are a bit larger and the estimated coefficient for x2 is a bit larger.

For high-leverage points, we need to find $\frac{2p}{n}$ and the leverage values to use the high-leverage value criterion: $h_{ii}>\frac{2p}{n}$:

```{r, include=TRUE}
lbound=(2*length(fit3$coefficients))/101
leverage=hatvalues(fit3)
which(leverage>lbound)
#Influential point
cooksd=cooks.distance(fit3)
influen=4/(101-length(fit3$coefficients))
which(cooksd>influen)
plot(cooksd, main ="Cook's distance", ylab="Cook's distance")
abline(h = 4/(101-length(fit3$coefficients)), col="red")
#Outlying in Y using studentized deleted Residuals
stu.res.del=studres(fit3)
head(sort(abs(stu.res.del),decreasing=TRUE))
#Compare Values to the Bonferroni Threshold
qt(1-.1/(2*101), 101-2-1)
```

According to the criterion and the "Residuals vs. Leverage plot" for model 2, the new observation(101) is a leverage point because its leverage value does satisfy $h_{ii}>\frac{2p}{n}$. It is not an outlier in Y because it lies within 3 standard deviations of zero and its studentized deleted residual does not exceed the Bonferroni threshold. The new observation is not an influential point according to cooks distance(its influence is smaller than $\frac{4}{n-p}$).









